import os
import json
import argparse
from tqdm import tqdm

from datasets_zoo import *
from annotator import OpenAIAnnotator


DATASET_CALL_FUNC = {
    'SODA': SodaDataset,
    'empathetic_dialogues': EmpathyDataset,
    'ConversationChronicles': ConversationChroniclesDataset,
    'Casino': CasinoDataset,
    'WildChat': WildChatDatset,
    'ConvAI2': ConvAI2Dataset,
    'Janus': MultifacetedCollectionDatset,
    'persuasion': PersuasionForGoodDataset,
    'wizard_of_wikipedia': WoWDataset,
    'cactus': CactusDataset,
    'MULTIWOZ2_2': MultiWoZDataset,
    'Prosocial': ProsocialDataset,
    'pearl': PearlDataset,
    'syn-personachat': SynPersonaChatDataset,
    'stark': StarkDataset
}

def main(args):
    """
    Main function to handle different modes of operation: 'execute' and 'parse'.
    
    Parameters:
    - args: Parsed command-line arguments.
    """
    
    # Process the dataset based on the specified name
    dataset = None
    if args.mode == 'execute':
        if args.dataset_name in DATASET_CALL_FUNC:
            dataset = DATASET_CALL_FUNC[args.dataset_name](args).prepare_data()
        else:
            raise ValueError(f"Unsupported dataset: {args.dataset_name}")
        
    # Initialize the annotator with the provided arguments and dataset
    annotator = OpenAIAnnotator(args, dataset=dataset)
        
    # Handle the mode of operation
    if args.mode == 'execute':
        annotator.run()
    elif args.mode == 'parse':
        output_path = os.path.join(args.batch_file_dir, args.dataset_name, f'{args.split}_batch_output.json')
        if not os.path.isfile(output_path):
            raise FileNotFoundError(f"Output file not found: {output_path}. Please ensure results are generated by OpenAI's LLM.")
        annotator.parse_generation(output_path)
    else:
        raise ValueError(f"Invalid mode: {args.mode}. Supported modes are 'execute' and 'parse'.")

if __name__ == '__main__':
    # Set up argument parser
    parser = argparse.ArgumentParser(description="Annotate multiple skills using OpenAI's LLM.")
    
    parser.add_argument('--dataset-name', required=True, type=str, help="Name of the dataset to process (e.g., 'SODA').")
    parser.add_argument('--split', required=True, type=str, help="Dataset split to use (e.g., 'train', 'test').")
    parser.add_argument('--model-name', required=True, type=str, help="Name of the model to use for annotation.")
    
    parser.add_argument('--debug', action='store_true', default=False, help="Enable debug mode for detailed logging.")
    parser.add_argument('--sub-sample-num', default=0, type=int)
    
    # Arguments specific to OpenAI's batch API
    parser.add_argument('--batch-file-dir', default='annotation_results', type=str, help="Directory where batch output files are stored.")
    parser.add_argument('--mode', required=True, type=str, choices=['execute', 'parse'], help="Operation mode: 'execute' to run annotations, 'parse' to parse results.")

    parser.add_argument('--multi-turn-filter', action='store_true', default=False, help="Filter multi-turn dialogues based on the number of turn.")
    parser.add_argument('--turn-num-threshold', default=0, type=int, help="Specify the threshold for the multi-turn filtering.")
    
    parser.add_argument('--target-language', default='English', type=str, help="Specify the target language of conversation.")
    # Parse arguments
    args = parser.parse_args()
    
    # Run main function with parsed arguments
    main(args)
